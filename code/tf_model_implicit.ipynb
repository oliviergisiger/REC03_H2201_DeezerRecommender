{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build\n",
    "\n",
    "<font color='red'>NOTE</font>\n",
    " \n",
    "* Notebook based on [this](https://blog.paperspace.com/movie-recommender-tensorflow/) tutorial. \n",
    "* see also [context feature implementation](https://www.tensorflow.org/recommenders/examples/context_features) and the corresponding [youtube video](https://www.youtube.com/watch?v=RWlLaWMD30M&t=1s)\n",
    "\n",
    "* Recommender runs, however only one recommendation for a given user_id, don't know why. \n",
    "\n",
    "To do:\n",
    "* Implement context features to take advantage of all the given variables\n",
    "<br>--> if we are able to do this, we might have something that would be viable for production (apparently this query-model-thing with consideration of context is the real-world approach), that would help for answer of Question 1 and 2 of the project description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from numpy import count_nonzero\n",
    "\n",
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7558834 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/hls1bn594jv01b8czjn52sp80000gn/T/ipykernel_53626/1628356132.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_listened_small[['user_id', 'media_id']] = train_listened_small[['user_id', 'media_id']].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# filter for is listened == 1\n",
    "\n",
    "train_listened = train_df[train_df['is_listened'] == 1]\n",
    "train_listened.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# only first 100'000 records\n",
    "train_listened_small = train_listened.loc[:99999]\n",
    "train_listened_small[['user_id', 'media_id']] = train_listened_small[['user_id', 'media_id']].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': b'16547', 'is_listened': 1, 'media_id': b'250467', 'timestamp': 1480544735}\n",
      "{'user_id': b'7665', 'is_listened': 1, 'media_id': b'305197', 'timestamp': 1479563953}\n",
      "{'user_id': b'1812', 'is_listened': 1, 'media_id': b'542335', 'timestamp': 1478368974}\n",
      "{'user_id': b'1812', 'is_listened': 1, 'media_id': b'542335', 'timestamp': 1478382544}\n",
      "{'user_id': b'1812', 'is_listened': 1, 'media_id': b'542335', 'timestamp': 1478338409}\n",
      "<MapDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'is_listened': TensorSpec(shape=(), dtype=tf.int64, name=None), 'media_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "# convert to tfds datset\n",
    "\n",
    "deezer_ratings = tf.data.Dataset.from_tensor_slices(dict(train_listened_small)).\\\n",
    "    map(lambda x: {\n",
    "    'user_id': x['user_id'], \n",
    "    'is_listened': x['is_listened'], \n",
    "    'media_id': x['media_id'], \n",
    "    'timestamp': x['ts_listen']})\n",
    "\n",
    "# get sample for overview\n",
    "for x in deezer_ratings.take(5).as_numpy_iterator():\n",
    "  print(x)\n",
    "\n",
    "# assert correct object type\n",
    "print(deezer_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = deezer_ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': b'1387', 'is_listened': 1, 'media_id': b'132123630', 'timestamp': 1479172239}\n",
      "{'user_id': b'572', 'is_listened': 1, 'media_id': b'130105294', 'timestamp': 1478282323}\n",
      "{'user_id': b'45', 'is_listened': 1, 'media_id': b'127539479', 'timestamp': 1479252882}\n",
      "{'user_id': b'2794', 'is_listened': 1, 'media_id': b'15417669', 'timestamp': 1478874840}\n",
      "{'user_id': b'15944', 'is_listened': 1, 'media_id': b'130105294', 'timestamp': 1480624462}\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "# for x in test.take(5).as_numpy_iterator():\n",
    "#   print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = deezer_ratings.map(lambda x: x[\"media_id\"])\n",
    "user = deezer_ratings.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "song_ids = deezer_ratings.batch(1_000_000).map(lambda x: x[\"media_id\"])\n",
    "user_ids = deezer_ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_song_ids = np.unique(np.concatenate(list(song_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "# context variable: unix timestamps\n",
    "# timestamps = np.concatenate(list(deezer_ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "# max_timestamp = timestamps.max()\n",
    "# min_timestamp = timestamps.min()\n",
    "\n",
    "# timestamp_buckets = np.linspace(\n",
    "#     min_timestamp, max_timestamp, num=1000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user and item models\n",
    "\n",
    "embedding_dimension = 32\n",
    "\n",
    "# Compute embeddings for users.\n",
    "user_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_user_ids, mask_token=None),\n",
    "    tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])\n",
    "\n",
    "# Compute embeddings for movies.\n",
    "song_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_song_ids, mask_token=None),\n",
    "    tf.keras.layers.Embedding(len(unique_song_ids) + 1, embedding_dimension)\n",
    "])\n",
    "\n",
    "# timestamp model: concatenate with user_model!\n",
    "# timestamp_embedding = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "#     tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "# ])\n",
    "# normalized_timestamp = tf.keras.layers.Normalization(\n",
    "#     axis=None\n",
    "# )\n",
    "\n",
    "# get top k recommendations\n",
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=song_ids.map(song_model)\n",
    ")\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeezerRecModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, song_model):\n",
    "    super().__init__()\n",
    "    self.song_model = tf.keras.Model = song_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    positive_song_embeddings = self.song_model(features[\"media_id\"])\n",
    "    return self.task(user_embeddings, positive_song_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 47s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.1275 - factorized_top_k/top_5_categorical_accuracy: 0.1283 - factorized_top_k/top_10_categorical_accuracy: 0.1333 - factorized_top_k/top_50_categorical_accuracy: 0.1758 - factorized_top_k/top_100_categorical_accuracy: 0.2167 - loss: 49878.8956 - regularization_loss: 0.0000e+00 - total_loss: 49878.8956\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 46s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.1098 - factorized_top_k/top_5_categorical_accuracy: 0.1128 - factorized_top_k/top_10_categorical_accuracy: 0.1225 - factorized_top_k/top_50_categorical_accuracy: 0.1803 - factorized_top_k/top_100_categorical_accuracy: 0.2291 - loss: 46373.7006 - regularization_loss: 0.0000e+00 - total_loss: 46373.7006\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 46s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.1084 - factorized_top_k/top_5_categorical_accuracy: 0.1134 - factorized_top_k/top_10_categorical_accuracy: 0.1268 - factorized_top_k/top_50_categorical_accuracy: 0.1927 - factorized_top_k/top_100_categorical_accuracy: 0.2469 - loss: 44666.4528 - regularization_loss: 0.0000e+00 - total_loss: 44666.4528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c260a00>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeezerRecModel(user_model, song_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.1365 - factorized_top_k/top_5_categorical_accuracy: 0.1365 - factorized_top_k/top_10_categorical_accuracy: 0.1400 - factorized_top_k/top_50_categorical_accuracy: 0.1660 - factorized_top_k/top_100_categorical_accuracy: 0.1920 - loss: 11566.0488 - regularization_loss: 0.0000e+00 - total_loss: 11566.0488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.13650000095367432,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.13650000095367432,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.14000000059604645,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.16599999368190765,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.19200000166893005,\n",
       " 'loss': 11566.048828125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 11566.048828125}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song id Recommendations for user: [b'132123630' b'132123630' b'132123630']\n"
     ]
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((songs.batch(1000), songs.batch(1000).map(model.song_model)))\n",
    ")\n",
    "\n",
    "_, titles = index(tf.constant([\"1387\"]))\n",
    "print(f\"Song id Recommendations for user: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deezer_recommendation-LXH-S8nB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a25f741d35dae877df10624e2b896bc7b21e32390eb0ff94c7989a0b15a6df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
